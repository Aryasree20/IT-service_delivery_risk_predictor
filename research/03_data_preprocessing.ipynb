{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614c8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed815e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IT-service_delivery_risk_predictor\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21846503",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c276036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IT-service_delivery_risk_predictor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c093b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: str\n",
    "    raw_data_path: str\n",
    "    pickle_save: str  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4ead27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_predictor.constants import *\n",
    "from risk_predictor.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353e642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.data_preprocessing\n",
    "        return DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            raw_data_path=config.raw_data_path,\n",
    "            pickle_save=config.pickle_save\n",
    "            \n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91944f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from risk_predictor import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b0f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.scaler = StandardScaler()\n",
    "        self.le = LabelEncoder()\n",
    "\n",
    "    def load_data(self, data_path: str):\n",
    "        \"\"\"Load raw CSV data\"\"\"\n",
    "        logger.info(f\"Loading data from {data_path}\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        logger.info(f\"Shape: {df.shape}\")\n",
    "        logger.info(f\"info: \\n{str(df.info())}\")\n",
    "        logger.info(f\"describe: \\n{df.describe().to_dict()}\")\n",
    "        logger.info(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "        logger.info(f\"Target distribution:\\n{df['predicted_risk'].value_counts().to_dict()}\")\n",
    "        return df\n",
    "\n",
    "    def validate_duration(self, df: pd.DataFrame):\n",
    "        \"\"\"Validate duration calculation\"\"\"\n",
    "        df[\"start_date\"] = pd.to_datetime(df[\"start_date\"])\n",
    "        df[\"end_date\"] = pd.to_datetime(df[\"end_date\"])\n",
    "        df['actual_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "        discrepancy = (df['actual_duration'] - df['actual_duration_days']).abs().sum()\n",
    "        if discrepancy != 0:\n",
    "            logger.warning(f\"Discrepancy in actual_duration_days: {discrepancy}\")\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"cross validate expected delivery_delay_days\"\"\"  \n",
    "        df[\"calculated_delay\"] = df[\"actual_duration_days\"] - df[\"planned_duration_days\"]\n",
    "        if (df[\"delivery_delay_days\"] != df[\"calculated_delay\"]).all():\n",
    "            logger.warning(f\"mismatch found\")\n",
    "      \n",
    "        return df\n",
    "    \n",
    "\n",
    "    def encode_target(self, df: pd.DataFrame):\n",
    "        \"\"\"Encode target labels\"\"\"\n",
    "        df['predicted_risk'] = self.le.fit_transform(df['predicted_risk'])\n",
    "        return df\n",
    "\n",
    "    def drop_columns(self, df: pd.DataFrame):\n",
    "        \"\"\"Drop unnecessary columns\"\"\"\n",
    "        df.drop(['project_id', 'start_date', 'end_date', 'delivery_delay_days','actual_duration','calculated_delay'], axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Split into train and test\"\"\"\n",
    "        X = df.drop('predicted_risk', axis=1)\n",
    "        y = df['predicted_risk']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        logger.info(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "        logger.info(f\"Class distribution before SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def apply_smote(self, X_train, y_train):\n",
    "        \"\"\"Apply SMOTE oversampling\"\"\"\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "        logger.info(f\"Class distribution after SMOTE: {pd.Series(y_res).value_counts().to_dict()}\")\n",
    "        return X_res, y_res\n",
    "\n",
    "    def scale_data(self, X_train, X_test):\n",
    "        \"\"\"Scale features\"\"\"\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "    \n",
    "    \n",
    "    def save_dataset(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Saves the processed train and test datasets as joblib files\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸ’¾ Saving train and test datasets as joblib files\")\n",
    "        pickle_save = self.config.pickle_save\n",
    "        os.makedirs(pickle_save, exist_ok=True)\n",
    "\n",
    "        # Paths\n",
    "        x_train_path = os.path.join(pickle_save, 'X_train.joblib')\n",
    "        x_test_path = os.path.join(pickle_save, 'X_test.joblib')\n",
    "        y_train_path = os.path.join(pickle_save, 'y_train.joblib')\n",
    "        y_test_path = os.path.join(pickle_save, 'y_test.joblib')\n",
    "\n",
    "        # Save\n",
    "        joblib.dump(X_train, x_train_path)\n",
    "        joblib.dump(X_test, x_test_path)\n",
    "        joblib.dump(y_train, y_train_path)\n",
    "        joblib.dump(y_test, y_test_path)\n",
    "\n",
    "        logger.info(f\" Datasets saved to {pickle_save}\")\n",
    "        return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8e2b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-07 16:39:43,641: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-07 16:39:43,652: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-10-07 16:39:43,652: INFO: common: created directory at: artifacts]\n",
      "[2025-10-07 16:39:43,656: INFO: 1942427899: Loading data from artifacts/data_ingestion/data.csv]\n",
      "[2025-10-07 16:39:43,756: INFO: 1942427899: Shape: (50000, 11)]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   project_id             50000 non-null  object \n",
      " 1   start_date             50000 non-null  object \n",
      " 2   end_date               50000 non-null  object \n",
      " 3   planned_duration_days  50000 non-null  int64  \n",
      " 4   actual_duration_days   50000 non-null  int64  \n",
      " 5   team_size              50000 non-null  int64  \n",
      " 6   num_bugs               50000 non-null  int64  \n",
      " 7   num_change_requests    50000 non-null  int64  \n",
      " 8   delivery_delay_days    50000 non-null  int64  \n",
      " 9   budget_overrun_pct     50000 non-null  float64\n",
      " 10  predicted_risk         50000 non-null  object \n",
      "dtypes: float64(1), int64(6), object(4)\n",
      "memory usage: 4.2+ MB\n",
      "[2025-10-07 16:39:43,777: INFO: 1942427899: info: \n",
      "None]\n",
      "[2025-10-07 16:39:43,822: INFO: 1942427899: describe: \n",
      "{'planned_duration_days': {'count': 50000.0, 'mean': 149.49804, 'std': 51.9300438161515, 'min': 60.0, '25%': 105.0, '50%': 149.0, '75%': 194.0, 'max': 239.0}, 'actual_duration_days': {'count': 50000.0, 'mean': 163.80524, 'std': 58.12767916478432, 'min': 30.0, '25%': 119.0, '50%': 164.0, '75%': 209.0, 'max': 298.0}, 'team_size': {'count': 50000.0, 'mean': 16.97426, 'std': 7.1887572651342415, 'min': 5.0, '25%': 11.0, '50%': 17.0, '75%': 23.0, 'max': 29.0}, 'num_bugs': {'count': 50000.0, 'mean': 49.47016, 'std': 28.837591815344677, 'min': 0.0, '25%': 24.0, '50%': 50.0, '75%': 74.0, 'max': 99.0}, 'num_change_requests': {'count': 50000.0, 'mean': 9.49814, 'std': 5.761095411881918, 'min': 0.0, '25%': 4.0, '50%': 10.0, '75%': 15.0, 'max': 19.0}, 'delivery_delay_days': {'count': 50000.0, 'mean': 14.3072, 'std': 26.069519760770646, 'min': -30.0, '25%': -8.0, '50%': 14.0, '75%': 37.0, 'max': 59.0}, 'budget_overrun_pct': {'count': 50000.0, 'mean': 0.250361, 'std': 0.2023141969800587, 'min': -0.1, '25%': 0.07, '50%': 0.25, '75%': 0.43, 'max': 0.6}}]\n",
      "[2025-10-07 16:39:43,838: INFO: 1942427899: Missing values:\n",
      "project_id               0\n",
      "start_date               0\n",
      "end_date                 0\n",
      "planned_duration_days    0\n",
      "actual_duration_days     0\n",
      "team_size                0\n",
      "num_bugs                 0\n",
      "num_change_requests      0\n",
      "delivery_delay_days      0\n",
      "budget_overrun_pct       0\n",
      "predicted_risk           0\n",
      "dtype: int64]\n",
      "[2025-10-07 16:39:43,849: INFO: 1942427899: Target distribution:\n",
      "{'High': 41922, 'Medium': 4320, 'Low': 3758}]\n",
      "[2025-10-07 16:39:43,903: INFO: 1942427899: Train shape: (40000, 6), Test shape: (10000, 6)]\n",
      "[2025-10-07 16:39:43,905: INFO: 1942427899: Class distribution before SMOTE: {0: 33563, 2: 3448, 1: 2989}]\n",
      "[2025-10-07 16:39:43,981: INFO: 1942427899: Class distribution after SMOTE: {0: 33563, 2: 33563, 1: 33563}]\n",
      "[2025-10-07 16:39:44,017: INFO: 1942427899: ðŸ’¾ Saving train and test datasets as joblib files]\n",
      "[2025-10-07 16:39:44,031: INFO: 1942427899:  Datasets saved to artifacts/data_preprocessing/pickle]\n",
      "[2025-10-07 16:39:44,039: INFO: 3944661174: Data Preprocessing pipeline executed successfully]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Arya\\anaconda3\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Arya\\anaconda3\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f4be' in position 44: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Arya\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Arya\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Arya\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\IT-service_delivery_risk_predictor\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Arya\\AppData\\Local\\Temp\\ipykernel_23296\\3944661174.py\", line 16, in <module>\n",
      "    data_preprocessing.save_dataset(X_train, X_test, y_train, y_test)\n",
      "  File \"C:\\Users\\Arya\\AppData\\Local\\Temp\\ipykernel_23296\\1942427899.py\", line 74, in save_dataset\n",
      "    logger.info(\"ðŸ’¾ Saving train and test datasets as joblib files\")\n",
      "Message: 'ðŸ’¾ Saving train and test datasets as joblib files'\n",
      "Arguments: ()\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_preprocessing_config = config.get_data_preprocessing_config()\n",
    "\n",
    "    # Initialize class\n",
    "    data_preprocessing = DataPreprocessing(config=data_preprocessing_config)\n",
    "\n",
    "    \n",
    "    df = data_preprocessing.load_data(data_preprocessing_config.raw_data_path)\n",
    "    df = data_preprocessing.validate_duration(df)\n",
    "    df = data_preprocessing.encode_target(df)\n",
    "    df = data_preprocessing.drop_columns(df)\n",
    "    X_train, X_test, y_train, y_test = data_preprocessing.split_data(df)\n",
    "    X_train, y_train = data_preprocessing.apply_smote(X_train, y_train)\n",
    "    X_train, X_test = data_preprocessing.scale_data(X_train, X_test)\n",
    "    data_preprocessing.save_dataset(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    logger.info(\"Data Preprocessing pipeline executed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"Error occurred during Data Preprocessing stage\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
